{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#homepage","title":"Homepage","text":""},{"location":"#install-vertex-cli-from-testpypi","title":"Install Vertex-CLI from TestPyPI","text":"<p>Vertex-CLI is a command-line tool that lets you query Large Language Models (LLMs) and debug faster, straight from your terminal.</p> <p>For example, you can run:</p> <pre><code>tex \"tell me about the solar system\"\n</code></pre> <p></p> <p>Replace <code>\"tell me about the solar system\"</code> with any query you like Vertex-CLI will generate a response using your selected LLM model.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install <code>Vertex-CLI</code> from TestPyPI, run:</p> <pre><code>pip install -i https://test.pypi.org/simple/ Vertex-CLI\n</code></pre> <p>Then initialize the CLI with:</p> <pre><code>tex --setup\n</code></pre>"},{"location":"#managing-api-keys-models","title":"Managing API Keys &amp; Models","text":"<ul> <li>Add a model and its API key:</li> </ul> <pre><code>tex config &lt;model-name&gt; &lt;api-key&gt;\n</code></pre> <ul> <li>Remove a model:</li> </ul> <pre><code>tex remove &lt;model-name&gt;\n</code></pre> <ul> <li>List all configured models:</li> </ul> <pre><code>tex list\n</code></pre> <ul> <li>Select a model to use by default:</li> </ul> <pre><code>tex select &lt;model-name&gt;\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Full CLI tool docs: CLI Tool Docs</li> <li>Contributor guide: Contributors Guide</li> <li>API reference: API Reference</li> </ul>"},{"location":"cli_tool_docs/","title":"Vertex CLI","text":"<p>Vertex CLI is a powerful command-line tool that leverages Large Language Models (LLMs) to answer queries and debug faster. With just a few commands, you can set up and start using advanced features like querying LLMs and generating insights.</p> <p>Complete Documentation: Vertex CLI Docs</p>"},{"location":"cli_tool_docs/#installation-and-setup","title":"Installation and Setup","text":"<p>Follow these steps to get started:</p>"},{"location":"cli_tool_docs/#install-vertex-cli-from-testpypi","title":"Install Vertex-CLI from TestPyPI","text":"<p>To install <code>Vertex-CLI</code> from TestPyPI, run:</p> <pre><code>pip install -i https://test.pypi.org/simple/ Vertex-CLI\n</code></pre> <p>After installation, initialize the CLI configuration file:</p> <pre><code>tex --setup\n</code></pre> <p>This will create the <code>models_api.json</code> under <code>~/.config/ai_model_manager/</code> with default entries.</p>"},{"location":"cli_tool_docs/#install-the-editable-version-for-development","title":"Install the Editable Version (For Development)","text":"<p>If you want to modify or contribute to Vertex CLI, install it in editable mode:</p> <ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/Prtm2110/Vertex-CLI\ncd Vertex-CLI\n</code></pre> <ol> <li>Install dependencies and set up the project:</li> </ol> <pre><code>pip install -e .\n</code></pre> <ol> <li>Initialize the CLI configuration:</li> </ol> <pre><code>tex --setup\n</code></pre>"},{"location":"cli_tool_docs/#configuration","title":"Configuration","text":"<p>You can configure the CLI to use a specific LLM model by adding or updating your API key:</p> <pre><code>tex config gemini-1.5-flash YOUR_MODEL_API_KEY\n</code></pre> <p>Replace <code>gemini-1.5-flash</code> with your preferred model name and <code>YOUR_MODEL_API_KEY</code> with your API key.</p> <p>To list all configured models:</p> <pre><code>tex list\n</code></pre> <p>To remove a model:</p> <pre><code>tex remove gemini-1.5-creative\n</code></pre> <p>To select a model as the default:</p> <pre><code>tex select gemini-1.5-flash\n</code></pre>"},{"location":"cli_tool_docs/#usage","title":"Usage","text":"<p>Once installed and configured, you can start chatting or debugging commands:</p>"},{"location":"cli_tool_docs/#quick-command-reference","title":"Quick Command Reference","text":"<ul> <li>Convert an array to a NumPy array</li> </ul> <pre><code>tex \"how to convert an array into a NumPy array\"\n</code></pre> <ul> <li> <p>Manage API keys for models</p> </li> <li> <p>Add or update a model\u2019s API key:</p> <p><pre><code>tex config &lt;model-name&gt; &lt;api-key&gt;\n</code></pre>   * Remove a model:</p> <p><pre><code>tex remove &lt;model-name&gt;\n</code></pre>   * List all saved models and their API keys:</p> <p><pre><code>tex list\n</code></pre>   * Select a model to use:</p> <p><pre><code>tex select &lt;model-name&gt;\n</code></pre>   * Show available commands/help:</p> <pre><code>tex -h\ntex chat -h\n</code></pre> </li> </ul>"},{"location":"cli_tool_docs/#debugging-beta-feature","title":"Debugging (Beta Feature)","text":"<ul> <li>Debug the last 3 commands (default):</li> </ul> <pre><code>tex debug\n</code></pre> <ul> <li>Debug a specific number of recent commands (e.g., last 5):</li> </ul> <pre><code>tex debug -n 5\n</code></pre> <ul> <li>Add a custom message to explain your assumptions or observations:</li> </ul> <pre><code>tex debug -n 5 -p \"I think this issue might be related to environment variables\"\n</code></pre> <p>\ud83d\udd17 Complete CLI Documentation: CLI Commands</p>"},{"location":"contributors_guide/","title":"Installing the Editable Version of Vertex-CLI","text":""},{"location":"contributors_guide/#1-fork-and-clone-the-repository","title":"1. Fork and Clone the Repository","text":"<p>First, fork the repository on GitHub. Then, clone it to your local machine:</p> <pre><code>git clone https://github.com/[YourUserName]/Vertex-CLI\ncd Vertex-CLI\n</code></pre>"},{"location":"contributors_guide/#2-install-in-editable-mode","title":"2. Install in Editable Mode","text":"<p>Run the following command to install the package in editable mode:</p> <pre><code>pip install -e .\n</code></pre> <p>This allows you to make changes to the code and use them immediately without reinstalling.</p>"},{"location":"contributors_guide/#3-initialize-the-cli","title":"3. Initialize the CLI","text":"<p>Run this command to set up the CLI and install necessary dependencies. It will add a free API key for you to get started:</p> <pre><code>tex-init\n</code></pre>"},{"location":"contributors_guide/#4-configure-the-cli-with-your-model-optional","title":"4. Configure the CLI with Your Model (Optional)","text":"<p>If you have your own model API key, you can configure it like this:</p> <pre><code>tex --config gemini-1.5-flash &lt;model-api-key&gt;\n</code></pre> <p>Replace <code>gemini-1.5-flash</code> with your model's name and <code>&lt;model-api-key&gt;</code> with your actual API key.</p>"},{"location":"references/","title":"API Reference","text":""},{"location":"references/#cli.ai_model_manager","title":"<code>ai_model_manager</code>","text":""},{"location":"references/#cli.ai_model_manager.AIModelManager","title":"<code>AIModelManager</code>","text":"<p>A class to manage AI model configurations stored in a JSON file. Allows adding, removing, selecting models, and generating output via LLM APIs.</p> Source code in <code>cli/ai_model_manager.py</code> <pre><code>class AIModelManager:\n    \"\"\"\n    A class to manage AI model configurations stored in a JSON file.\n    Allows adding, removing, selecting models, and generating output via LLM APIs.\n    \"\"\"\n\n    def __init__(self, file_path=None):\n        \"\"\"\n        Initialize the AIModelManager instance.\n        Creates the default configuration file if it doesn't exist.\n\n        Args:\n            file_path (str, optional): Path to the JSON configuration file.\n                If None, defaults to ~/.config/ai_model_manager/models_api.json.\n        \"\"\"\n        if file_path is None:\n            config_dir = os.path.join(\n                os.path.expanduser(\"~\"), \".config\", \"ai_model_manager\"\n            )\n            os.makedirs(config_dir, exist_ok=True)\n            file_path = os.path.join(config_dir, \"models_api.json\")\n\n        self.file_path = file_path\n\n        if not os.path.exists(self.file_path):\n            self.create_default_file()\n\n    def _read_json(self):\n        \"\"\"\n        Internal method to read JSON configuration data from file.\n\n        Returns:\n            dict: Parsed JSON data or an empty dict if file not found.\n        \"\"\"\n        try:\n            with open(self.file_path, \"r\") as f:\n                return json.load(f)\n        except FileNotFoundError:\n            return {}\n\n    def _write_json(self, data):\n        \"\"\"\n        Internal method to write JSON data to file.\n\n        Args:\n            data (dict): Data to write into the JSON file.\n        \"\"\"\n        with open(self.file_path, \"w\") as f:\n            json.dump(data, f, indent=4)\n\n    def create_default_file(self):\n        \"\"\"\n        Creates a configuration file with default model entries.\n        Includes a pre-set API key for `gemini-1.5-flash`.\n        \"\"\"\n        default_config = {\n            \"selected_model\": None,\n            \"gemini-1.5-flash\": \"AIzaSyCSXtRAITXfGuarMHI1j-0QyKkoT9mUfz8\",\n            \"gemini-1.5-interactive\": None,\n            \"gemini-1.5-creative\": None,\n        }\n        self._write_json(default_config)\n        print(f\"Config file created at: {self.file_path}\")\n\n    def load(self):\n        \"\"\"\n        Loads the current configuration from the JSON file.\n\n        Returns:\n            dict: Configuration data including selected model and keys.\n        \"\"\"\n        return self._read_json()\n\n    def configure_model(self, model_name, api_key):\n        \"\"\"\n        Adds or updates a model entry with a given API key.\n\n        Args:\n            model_name (str): The name of the model to configure.\n            api_key (str): The API key associated with the model.\n        \"\"\"\n        data = self._read_json()\n        data[model_name] = api_key\n        self._write_json(data)\n        print(\"Model added successfully.\")\n\n    def remove_model(self, model_name):\n        \"\"\"\n        Removes a model from the configuration.\n\n        Args:\n            model_name (str): The name of the model to remove.\n\n        Raises:\n            ValueError: If the model is not found in the configuration.\n        \"\"\"\n        data = self._read_json()\n        if model_name in data:\n            del data[model_name]\n            self._write_json(data)\n            print(f\"Model '{model_name}' removed successfully.\")\n        else:\n            raise ValueError(f\"Model '{model_name}' not found.\")\n\n    def get_api_key(self, model_name):\n        \"\"\"\n        Retrieves the API key for a given model.\n\n        Args:\n            model_name (str): The name of the model.\n\n        Returns:\n            str: The API key associated with the model.\n\n        Raises:\n            ValueError: If the model is not available or lacks an API key.\n        \"\"\"\n        data = self._read_json()\n        if model_name in data and data[model_name]:\n            return data[model_name]\n        raise ValueError(f\"Model '{model_name}' is not available or has no API key.\")\n\n    def list_models(self):\n        \"\"\"\n        Prints the list of models and their corresponding API keys (if available).\n        \"\"\"\n        data = self._read_json()\n        if data:\n            for model, key in data.items():\n                print(f\"{model}: {key}\")\n        else:\n            print(\"No models found.\")\n\n    def select_model(self, model_name):\n        \"\"\"\n        Sets the specified model as the default selected model.\n\n        Args:\n            model_name (str): The name of the model to select.\n\n        Prints a message based on whether the selection was successful.\n        \"\"\"\n        data = self._read_json()\n        if model_name in data and data[model_name]:\n            data[\"selected_model\"] = model_name\n            self._write_json(data)\n            print(f\"Selected model: {model_name}\")\n        else:\n            print(\"No models found or no API key for that model.\")\n\n    def generate_output(self, model_name, prompt_by_user):\n        \"\"\"\n        Generates LLM output using the specified model and user prompt.\n\n        This method displays a spinner while waiting for a response from the model.\n\n        Args:\n            model_name (str): The name of the model to use.\n            prompt_by_user (str): The prompt to send to the model.\n\n        Returns:\n            str: The model's output.\n        \"\"\"\n        stop_spinner = threading.Event()\n        spinner_thread = threading.Thread(target=spin_loader, args=(stop_spinner,))\n        spinner_thread.start()\n\n        from cli.llm import gemini_api_output\n\n        output = gemini_api_output(model_name, prompt_by_user)\n\n        stop_spinner.set()\n        spinner_thread.join()\n\n        return output\n</code></pre>"},{"location":"references/#cli.ai_model_manager.AIModelManager.__init__","title":"<code>__init__(file_path=None)</code>","text":"<p>Initialize the AIModelManager instance. Creates the default configuration file if it doesn't exist.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the JSON configuration file. If None, defaults to ~/.config/ai_model_manager/models_api.json.</p> <code>None</code> Source code in <code>cli/ai_model_manager.py</code> <pre><code>def __init__(self, file_path=None):\n    \"\"\"\n    Initialize the AIModelManager instance.\n    Creates the default configuration file if it doesn't exist.\n\n    Args:\n        file_path (str, optional): Path to the JSON configuration file.\n            If None, defaults to ~/.config/ai_model_manager/models_api.json.\n    \"\"\"\n    if file_path is None:\n        config_dir = os.path.join(\n            os.path.expanduser(\"~\"), \".config\", \"ai_model_manager\"\n        )\n        os.makedirs(config_dir, exist_ok=True)\n        file_path = os.path.join(config_dir, \"models_api.json\")\n\n    self.file_path = file_path\n\n    if not os.path.exists(self.file_path):\n        self.create_default_file()\n</code></pre>"},{"location":"references/#cli.ai_model_manager.AIModelManager.configure_model","title":"<code>configure_model(model_name, api_key)</code>","text":"<p>Adds or updates a model entry with a given API key.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to configure.</p> required <code>api_key</code> <code>str</code> <p>The API key associated with the model.</p> required Source code in <code>cli/ai_model_manager.py</code> <pre><code>def configure_model(self, model_name, api_key):\n    \"\"\"\n    Adds or updates a model entry with a given API key.\n\n    Args:\n        model_name (str): The name of the model to configure.\n        api_key (str): The API key associated with the model.\n    \"\"\"\n    data = self._read_json()\n    data[model_name] = api_key\n    self._write_json(data)\n    print(\"Model added successfully.\")\n</code></pre>"},{"location":"references/#cli.ai_model_manager.AIModelManager.create_default_file","title":"<code>create_default_file()</code>","text":"<p>Creates a configuration file with default model entries. Includes a pre-set API key for <code>gemini-1.5-flash</code>.</p> Source code in <code>cli/ai_model_manager.py</code> <pre><code>def create_default_file(self):\n    \"\"\"\n    Creates a configuration file with default model entries.\n    Includes a pre-set API key for `gemini-1.5-flash`.\n    \"\"\"\n    default_config = {\n        \"selected_model\": None,\n        \"gemini-1.5-flash\": \"AIzaSyCSXtRAITXfGuarMHI1j-0QyKkoT9mUfz8\",\n        \"gemini-1.5-interactive\": None,\n        \"gemini-1.5-creative\": None,\n    }\n    self._write_json(default_config)\n    print(f\"Config file created at: {self.file_path}\")\n</code></pre>"},{"location":"references/#cli.ai_model_manager.AIModelManager.generate_output","title":"<code>generate_output(model_name, prompt_by_user)</code>","text":"<p>Generates LLM output using the specified model and user prompt.</p> <p>This method displays a spinner while waiting for a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to use.</p> required <code>prompt_by_user</code> <code>str</code> <p>The prompt to send to the model.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The model's output.</p> Source code in <code>cli/ai_model_manager.py</code> <pre><code>def generate_output(self, model_name, prompt_by_user):\n    \"\"\"\n    Generates LLM output using the specified model and user prompt.\n\n    This method displays a spinner while waiting for a response from the model.\n\n    Args:\n        model_name (str): The name of the model to use.\n        prompt_by_user (str): The prompt to send to the model.\n\n    Returns:\n        str: The model's output.\n    \"\"\"\n    stop_spinner = threading.Event()\n    spinner_thread = threading.Thread(target=spin_loader, args=(stop_spinner,))\n    spinner_thread.start()\n\n    from cli.llm import gemini_api_output\n\n    output = gemini_api_output(model_name, prompt_by_user)\n\n    stop_spinner.set()\n    spinner_thread.join()\n\n    return output\n</code></pre>"},{"location":"references/#cli.ai_model_manager.AIModelManager.get_api_key","title":"<code>get_api_key(model_name)</code>","text":"<p>Retrieves the API key for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The API key associated with the model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is not available or lacks an API key.</p> Source code in <code>cli/ai_model_manager.py</code> <pre><code>def get_api_key(self, model_name):\n    \"\"\"\n    Retrieves the API key for a given model.\n\n    Args:\n        model_name (str): The name of the model.\n\n    Returns:\n        str: The API key associated with the model.\n\n    Raises:\n        ValueError: If the model is not available or lacks an API key.\n    \"\"\"\n    data = self._read_json()\n    if model_name in data and data[model_name]:\n        return data[model_name]\n    raise ValueError(f\"Model '{model_name}' is not available or has no API key.\")\n</code></pre>"},{"location":"references/#cli.ai_model_manager.AIModelManager.list_models","title":"<code>list_models()</code>","text":"<p>Prints the list of models and their corresponding API keys (if available).</p> Source code in <code>cli/ai_model_manager.py</code> <pre><code>def list_models(self):\n    \"\"\"\n    Prints the list of models and their corresponding API keys (if available).\n    \"\"\"\n    data = self._read_json()\n    if data:\n        for model, key in data.items():\n            print(f\"{model}: {key}\")\n    else:\n        print(\"No models found.\")\n</code></pre>"},{"location":"references/#cli.ai_model_manager.AIModelManager.load","title":"<code>load()</code>","text":"<p>Loads the current configuration from the JSON file.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>Configuration data including selected model and keys.</p> Source code in <code>cli/ai_model_manager.py</code> <pre><code>def load(self):\n    \"\"\"\n    Loads the current configuration from the JSON file.\n\n    Returns:\n        dict: Configuration data including selected model and keys.\n    \"\"\"\n    return self._read_json()\n</code></pre>"},{"location":"references/#cli.ai_model_manager.AIModelManager.remove_model","title":"<code>remove_model(model_name)</code>","text":"<p>Removes a model from the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to remove.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is not found in the configuration.</p> Source code in <code>cli/ai_model_manager.py</code> <pre><code>def remove_model(self, model_name):\n    \"\"\"\n    Removes a model from the configuration.\n\n    Args:\n        model_name (str): The name of the model to remove.\n\n    Raises:\n        ValueError: If the model is not found in the configuration.\n    \"\"\"\n    data = self._read_json()\n    if model_name in data:\n        del data[model_name]\n        self._write_json(data)\n        print(f\"Model '{model_name}' removed successfully.\")\n    else:\n        raise ValueError(f\"Model '{model_name}' not found.\")\n</code></pre>"},{"location":"references/#cli.ai_model_manager.AIModelManager.select_model","title":"<code>select_model(model_name)</code>","text":"<p>Sets the specified model as the default selected model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to select.</p> required <p>Prints a message based on whether the selection was successful.</p> Source code in <code>cli/ai_model_manager.py</code> <pre><code>def select_model(self, model_name):\n    \"\"\"\n    Sets the specified model as the default selected model.\n\n    Args:\n        model_name (str): The name of the model to select.\n\n    Prints a message based on whether the selection was successful.\n    \"\"\"\n    data = self._read_json()\n    if model_name in data and data[model_name]:\n        data[\"selected_model\"] = model_name\n        self._write_json(data)\n        print(f\"Selected model: {model_name}\")\n    else:\n        print(\"No models found or no API key for that model.\")\n</code></pre>"},{"location":"references/#cli.llm","title":"<code>llm</code>","text":""},{"location":"references/#cli.llm.gemini_api_output","title":"<code>gemini_api_output(model_name, prompt_by_user)</code>","text":"<p>Generate AI response using specified model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the AI model to use.</p> required <code>prompt_by_user</code> <code>str</code> <p>User's input prompt.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Generated response text.</p> Source code in <code>cli/llm.py</code> <pre><code>def gemini_api_output(model_name, prompt_by_user):\n    \"\"\"\n    Generate AI response using specified model.\n\n    Args:\n        model_name (str): Name of the AI model to use.\n        prompt_by_user (str): User's input prompt.\n\n    Returns:\n        str: Generated response text.\n    \"\"\"\n    from cli.ai_model_manager import AIModelManager\n\n    manager = AIModelManager()\n    api_key = manager.get_api_key(model_name)\n\n    client = genai.Client(api_key=api_key)\n    response = client.models.generate_content(model=model_name, contents=prompt_by_user)\n\n    return response.text\n</code></pre>"},{"location":"references/#cli.prettify_llm_output","title":"<code>prettify_llm_output</code>","text":""},{"location":"references/#cli.prettify_llm_output.prettify_llm_output","title":"<code>prettify_llm_output(response)</code>","text":"<p>Prettifies the output from a language model response by stripping leading and trailing whitespace and code block markers, then prints it as Markdown to the console.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>The raw response from the language model.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>cli/prettify_llm_output.py</code> <pre><code>def prettify_llm_output(response):\n    \"\"\"\n    Prettifies the output from a language model response by stripping leading\n    and trailing whitespace and code block markers, then prints it as Markdown\n    to the console.\n\n    Args:\n        response (str): The raw response from the language model.\n\n    Returns:\n        None\n    \"\"\"\n    markdown_output = response.strip().strip(\"```\")\n    console = Console()\n    md = Markdown(markdown_output)\n    print()\n    console.print(md)\n    print()\n</code></pre>"},{"location":"references/#cli.utils","title":"<code>utils</code>","text":""},{"location":"references/#cli.utils.install_requirements","title":"<code>install_requirements()</code>","text":"<p>Installs the required dependencies for the application.</p> Source code in <code>cli/utils.py</code> <pre><code>def install_requirements():\n    \"\"\"\n    Installs the required dependencies for the application.\n    \"\"\"\n\n    dependencies = [\"rich==14.0.0\", \"google-genai\"]\n    for package in dependencies:\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package])\n</code></pre>"},{"location":"references/#cli.utils.spin_loader","title":"<code>spin_loader(stop_event)</code>","text":"<p>Displays a spinning loader in the console until the stop_event is set.</p> <p>Parameters:</p> Name Type Description Default <code>stop_event</code> <code>Event</code> <p>An event object used to signal the loader to stop.</p> required Source code in <code>cli/utils.py</code> <pre><code>def spin_loader(stop_event):\n    \"\"\"\n    Displays a spinning loader in the console until the stop_event is set.\n\n    Args:\n        stop_event (threading.Event): An event object used to signal the loader to stop.\n    \"\"\"\n    spinner = itertools.cycle([\"-\", \"/\", \"|\", \"\\\\\"])\n    while not stop_event.is_set():\n        sys.stdout.write(next(spinner))\n        sys.stdout.flush()\n        time.sleep(0.1)\n        sys.stdout.write(\"\\b\")\n    sys.stdout.write(\" \")\n    sys.stdout.flush()\n</code></pre>"}]}